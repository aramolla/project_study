from langchain_openai import OpenAIEmbeddings, ChatOpenAI
import streamlit as st
import os

# Set up page configuration
st.set_page_config(
    page_title="ìƒëª…ëŒ€í•™êµ Chat bot", 
    page_icon="/Users/aramolla/Desktop/project/ìˆ˜ë­‰1.png",  # ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
    layout="wide"
)
st.title("AI ìˆ˜ë­‰ ğŸ’¬")

# CSS to inject contained in a string
custom_css = """
            <style>
            /* Hide Streamlit style elements */
            #MainMenu {visibility: hidden;}
            footer {visibility: hidden;}
            header {visibility: hidden;}

            /* General page background */
            body {
                background-color: #f5f5f5;
                color: #333333;
            }

            /* Sidebar customization */
            .css-1d391kg {
                background-color: #ffffff !important;
                border-right: 1px solid #e0e0e0;
            }

            /* Input field styling */
            .stTextInput>div>div>div>input {
                padding: 10px 20px;
                border-radius: 10px;
                border: 1px solid #ccc;
                background-color: #ffffff;
                color: #333333;
            }

            /* Button styling */
            .stButton>button {
                border-radius: 10px;
                padding: 10px 20px;
                border: none;
                background-color: #007bff;
                color: #ffffff;
                font-weight: bold;
            }
            .stButton>button:hover {
                background-color: #0056b3;
                color: #ffffff;
            }

            /* Chat message styling */
            .stChatMessage {
                border: 1px solid #e0e0e0;
                border-radius: 10px;
                padding: 10px;
                color: #000000; /* í…ìŠ¤íŠ¸ ìƒ‰ìƒì„ ê²€ì •ìœ¼ë¡œ ë³€ê²½ */
                margin: 10px 0;
                max-width: 60%; /* ê°€ë¡œ ê³µê°„ì˜ 60%ë§Œ ì°¨ì§€ */
                display: flex;
                align-items: center;
            }

            .stChatMessage-user {
                background-color: #e9ecef;
                color: #000000; /* í…ìŠ¤íŠ¸ ìƒ‰ìƒì„ ê²€ì •ìœ¼ë¡œ ë³€ê²½ */
                text-align: right; /* í…ìŠ¤íŠ¸ ì˜¤ë¥¸ìª½ ì •ë ¬ */
                margin-left: auto; /* ì™¼ìª½ ë§ˆì§„ì„ ìë™ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì˜¤ë¥¸ìª½ ì •ë ¬ */
                justify-content: flex-end; /* ì½˜í…ì¸ ë¥¼ ì˜¤ë¥¸ìª½ ì •ë ¬ */
            }

            .stChatMessage-assistant {
                background-color: #f8f9fa;
                color: #000000; /* í…ìŠ¤íŠ¸ ìƒ‰ìƒì„ ê²€ì •ìœ¼ë¡œ ë³€ê²½ */
                text-align: left; /* í…ìŠ¤íŠ¸ ì™¼ìª½ ì •ë ¬ */
                margin-right: auto; /* ì˜¤ë¥¸ìª½ ë§ˆì§„ì„ ìë™ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì™¼ìª½ ì •ë ¬ */
                justify-content: flex-start; /* ì½˜í…ì¸ ë¥¼ ì™¼ìª½ ì •ë ¬ */
            }


            </style>
            """

st.markdown(custom_css, unsafe_allow_html=True)

# OpenAI API Key ì„¤ì •
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

# í˜ì´ì§€ê°€ ì„¸ë¡œê³ ì¹¨ì´ ì¼ì–´ë‚˜ë„ ì´ì „ ë©”ì‹œì§€ê°€ ìŒ“ì´ë„ë¡ ë°ì´í„° ë³´ê´€
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# ì±„íŒ… ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•˜ëŠ” Store ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜
if "store" not in st.session_state:
    st.session_state["store"] = dict()

with st.sidebar:
    st.image("/Users/aramolla/Desktop/project/ìˆ˜ë­‰_2.png", width=200)

    session_id = st.text_input("Session ID", value="ara123")
    clear_btn = st.button("ì´ˆê¸°í™”")
    if clear_btn:
        st.session_state["messages"] = []
        if session_id in st.session_state["store"]:
            del st.session_state["store"][session_id]  # íŠ¹ì • ì„¸ì…˜ IDì— ëŒ€í•œ ê¸°ë¡ë§Œ ì‚­ì œ
        st.experimental_rerun()


from utils import print_messages, StreamHandler

print_messages()  # ì´ì „ ëŒ€í™”ë‚´ìš© ì¶œë ¥

from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory

# ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in st.session_state["store"]:  # ì„¸ì…˜ IDê°€ storeì— ì—†ëŠ” ê²½ìš°
        st.session_state["store"][session_id] = ChatMessageHistory()  # ìƒˆë¡œìš´ ChatMessageHistory ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ Storeì— ì €ì¥
    return st.session_state["store"][session_id]  # í•´ë‹¹ ì„¸ì…˜ IDì— ëŒ€í•œ ì„¸ì…˜ ê¸°ë¡ ë°˜í™˜

from langchain_community.vectorstores import FAISS
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.base import RunnableSequence, RunnableLambda
from langchain_core.runnables.history import RunnableWithMessageHistory

def trim_messages(messages, max_tokens=16000): # ì¶”ê°€
    total_tokens = 0
    trimmed_messages = []
    for message in reversed(messages):
        total_tokens += len(message[1].split())
        if total_tokens <= max_tokens:
            trimmed_messages.append(message)
        else:
            break
    return list(reversed(trimmed_messages))

# ì¤‘ë³µëœ ë¬¸ì¥ì„ ì œê±°í•˜ëŠ” í•¨ìˆ˜
def remove_duplicates(response_list):
    seen = set()
    result = []
    for response in response_list:
        if response not in seen:
            seen.add(response)
            result.append(response)
    return result

if user_input := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”."):
    st.session_state["messages"].append(("user", user_input))
    st.chat_message("user").write(f"{user_input}")

    # with st.chat_message("assistant"):
    container = st.empty()
    stream_handler = StreamHandler(container)

    # ëª¨ë¸ ìƒì„±
    GPT_4o = ChatOpenAI(model="gpt-3.5-turbo-1106", streaming=True, callbacks=[stream_handler]) # "gpt-4o-2024-05-13"    16,385	
    embeddings = OpenAIEmbeddings(model="text-embedding-3-large")

    # ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
    def load_vector_store(path):
        return FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)

    # pdfë•Œë©” ì£¼ì„í•´ë‘ 
    vector_store_path = "/Users/aramolla/Desktop/project/vector_store_ê³µì§€ì‚¬í•­"
    db = load_vector_store(vector_store_path)

    # from langchain.text_splitter import RecursiveCharacterTextSplitter
    # from langchain.document_loaders import PyPDFLoader

    # loader = PyPDFLoader("/Users/aramolla/Desktop/project/ê¸€ë¡œë²Œ1.pdf")
    # pages = loader.load_and_split()

    # text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    # texts = text_splitter.split_documents(pages)

    # db = FAISS.from_documents(texts, embeddings)


    from langchain.retrievers.multi_query import MultiQueryRetriever

    # MultiQueryRetriever ì„¤ì •
    retriever_from_llm = MultiQueryRetriever.from_llm(
        retriever=db.as_retriever(
            search_type="similarity",
            search_kwargs={'k': 5}#  50 -> 5
        ), llm=GPT_4o
    )

    #ì¿¼ë¦¬ ë¡œê¹… í•´ì£¼ê¸°
    import logging

    logging.basicConfig()
    logging.getLogger("langchain.retrievers.multi_query").setLevel(logging.INFO)


    # retriever = db.as_retriever(
    #     search_type="similarity",
    #     search_kwargs={'k': 5}#  50 -> 5
    # )


    prompt_template = """
    ### [INST]
    AIìˆ˜ë­‰ì…ë‹ˆë‹¤! ê¶ê¸ˆí•œê±¸ ë¬¼ì–´ë³´ì‡¼

    {context}

    ### ì§ˆë¬¸:
    {question}

    [/INST]
    ë‹µë³€!
    """

    # í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", 
                """
                ì—­í• : ë„ˆëŠ” ë°˜ë§ë¡œ ë‹µë³€í•˜ëŠ” ìƒëª…ëŒ€í•™êµ í•™ìƒë“¤ì˜ í•™êµ ìƒí™œì„ ë„ì™€ì¤„ ë„ìš°ë¯¸ì•¼.
                ì´ë¦„: ë„ˆì˜ ì´ë¦„ì€ 'ìˆ˜ë­‰'ì´ì•¼.
                ì‚¬ìš©ì í˜¸ì¹­: ë„ˆê°€ userë¥¼ ë¶€ë¥¼ ë•ŒëŠ” 'ìŠ´ìš°'ë¼ê³  ë¶ˆëŸ¬ì•¼ ë¼.
                """
            ),
            MessagesPlaceholder(variable_name="history"),
            ("human", prompt_template.format(context="{context}", question="{question}")),
        ]
    )

    # LLM ì²´ì¸ ìƒì„±
    llm_chain = prompt | GPT_4o

    # rag_chain ì •ì˜ ìˆ˜ì •
    rag_chain = RunnableSequence(
        RunnableLambda(lambda x: {
            "context": [doc.page_content for doc in retriever_from_llm.get_relevant_documents(x["question"])],
            "question": x["question"],
            "history": x["history"]  # ì¶”ê°€ëœ ë¶€ë¶„
        }),
        llm_chain
    )

    chain_with_memory = RunnableWithMessageHistory(
        rag_chain,
        lambda session_id: get_session_history(session_id),
        input_messages_key="question",
        history_messages_key="history",
    )


    # ì´ì „ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì¶”ì í•˜ê³  ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
    def get_relevant_context(history):
        if not history:
            return ""
        for message in reversed(history):
            if message[0] == "assistant":
                return message[1]
        return ""

    # ì´ì „ ë‹µë³€ì—ì„œ ë§¥ë½ì„ ì°¾ìŒ
    context = get_relevant_context(st.session_state["messages"])
    context_list = context.split('\n')
    context_list = remove_duplicates(context_list)
    context = '\n'.join(context_list)

    # ì±„íŒ… ê¸°ë¡ì„ íŠ¸ë¦¬ë°í•˜ì—¬ ìµœëŒ€ í† í° ê¸¸ì´ ì œí•œì„ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ í•¨
    trimmed_messages = trim_messages(st.session_state["messages"])
    trimmed_messages = [(role, remove_duplicates([message])[0]) for role, message in trimmed_messages]
    

    relevant_docs = retriever_from_llm.get_relevant_documents(user_input)
    detailed_info = None



    similar_questions = [doc.page_content for doc in relevant_docs]
    combined_response = "\n".join([f"{i+1}. {q}" for i, q in enumerate(similar_questions)])

    # ì¤‘ë³µëœ ë¬¸ì¥ì„ ì œê±°í•˜ëŠ” í•¨ìˆ˜
    def remove_duplicates(response_list):
        seen = set()
        result = []
        for response in response_list:
            if response not in seen:
                seen.add(response)
                result.append(response)
        return result

    # chain_with_memory.invoke í˜¸ì¶œ ë¶€ë¶„ ìˆ˜ì •
    response = chain_with_memory.invoke(
        {"question": user_input, "history": trimmed_messages, "context": context},  # historyì™€ context ì¶”ê°€ # trimmed_messages -> st.session_state["messages"]
        config={'configurable': {'session_id': session_id}}
    )

    msg = response.content  # ì†ì„±ìœ¼ë¡œ ì ‘ê·¼

    # ì¤‘ë³µ ë¬¸ì¥ì„ ì œê±°
    msg_list = msg.split('\n')
    msg_list = remove_duplicates(msg_list)
    final_msg = '\n'.join(msg_list)

    # st.write(msg)
    st.session_state["messages"].append(("assistant", final_msg))


    for role, message in st.session_state["messages"]:
        st.chat_message(role).write(message)
