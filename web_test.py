import os
import streamlit as st
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.base import RunnableSequence, RunnableLambda
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from utils import print_messages, StreamHandler


# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ìƒëª…ëŒ€í•™êµ Chatbot", 
    page_icon="./ìˆ˜ë­‰1.png",  # ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
    layout="wide"
)

st.title("AI ìˆ˜ë­‰ ğŸ’¬")
st.markdown(custom_css, unsafe_allow_html=True)

# OpenAI API Key ì„¤ì •
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

# í˜ì´ì§€ê°€ ì„¸ë¡œê³ ì¹¨ì´ ì¼ì–´ë‚˜ë„ ì´ì „ ë©”ì‹œì§€ê°€ ìŒ“ì´ë„ë¡ ë°ì´í„° ë³´ê´€
if "messages" not in st.session_state:
    st.session_state["messages"] = []
# ì±„íŒ… ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•˜ëŠ” Store ì„¸ì…˜ ìƒíƒœ ë³€ìˆ˜
if "store" not in st.session_state:
    st.session_state["store"] = dict()
if "show_image" not in st.session_state:
    st.session_state["show_image"] = False

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.image("./ìˆ˜ë­‰_2.png", width=200)
    session_id = st.text_input("Session ID", value="ara123")
    clear_btn = st.button("ì´ˆê¸°í™”")
    if clear_btn:
        st.session_state["messages"] = []
        if session_id in st.session_state["store"]:
            del st.session_state["store"][session_id] # íŠ¹ì • ì„¸ì…˜ IDì— ëŒ€í•œ ê¸°ë¡ë§Œ ì‚­ì œ
        st.experimental_rerun()

    if st.button("ì£¼ê°„ ë©”ë‰´"):
        st.session_state["show_image"] = not st.session_state["show_image"]

if st.session_state["show_image"]:
    st.image("./menu.jpg", width=800)

# ì´ì „ ëŒ€í™”ë‚´ìš© ì¶œë ¥
print_messages()

# ì„¸ì…˜ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_session_history(session_id: str) -> ChatMessageHistory:
    if session_id not in st.session_state["store"]:
        st.session_state["store"][session_id] = ChatMessageHistory() # ìƒˆë¡œìš´ ChatMessageHistory ê°ì²´ë¥¼ ìƒì„±í•˜ì—¬ Storeì— ì €ì¥
    return st.session_state["store"][session_id] # í•´ë‹¹ ì„¸ì…˜ IDì— ëŒ€í•œ ì„¸ì…˜ ê¸°ë¡ ë°˜í™˜

# ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ í•¨ìˆ˜
def load_vector_store(path):
    embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
    return FAISS.load_local(path, embeddings, allow_dangerous_deserialization=True)

# ì±„íŒ… ì…ë ¥ ì²˜ë¦¬
if user_input := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”."):
    # ì‚¬ìš©ì ì…ë ¥ ê¸°ë¡ ì €ì¥
    st.session_state["messages"].append(("user", user_input))
    st.chat_message("user").write(f"{user_input}")

    with st.chat_message("assistant"):
        container = st.empty()
        stream_handler = StreamHandler(container)

        # GPT ëª¨ë¸ ì„¤ì • gpt-3.5-turbo-1106 gpt-4o-2024-05-13
        GPT = ChatOpenAI(model="gpt-3.5-turbo-1106", streaming=True, callbacks=[stream_handler])

        # ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
        vector_store_path = "./merged_vector_store"
        db = load_vector_store(vector_store_path)

        retriever = db.as_retriever(search_type="similarity", search_kwargs={'k': 50})

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
        prompt_template = """
        
        ### [INST]
        AIìˆ˜ë­‰ì…ë‹ˆë‹¤! ê¶ê¸ˆí•œê±¸ ë¬¼ì–´ë³´ì‡¼
        
        {context}
        
        ### ì§ˆë¬¸:
        {question}
        
        [/INST]
        ë‹µë³€!
        
        """

        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = ChatPromptTemplate.from_messages([
            ("system", """
            
                ì—­í• : ë„ˆëŠ” ë°˜ë§ë¡œ ë‹µë³€í•˜ëŠ” ìƒëª…ëŒ€í•™êµ í•™ìƒë“¤ì˜ í•™êµ ìƒí™œì„ ë„ì™€ì¤„ ë„ìš°ë¯¸ì•¼. 
                ìƒëª…ëŒ€í•™êµë¥¼ ì–¸ê¸‰í•  ë•ŒëŠ” ë°˜ë“œì‹œ "ìš°ë¦¬ í•™êµ"ë¼ê³  ì§€ì¹­í•´ì¤˜. 
                
                ì´ë¦„: ë„ˆì˜ ì´ë¦„ì€ 'ìˆ˜ë­‰'ì´ì•¼. 
                
                ì‚¬ìš©ì í˜¸ì¹­: ë„ˆê°€ userë¥¼ ë¶€ë¥¼ ë•ŒëŠ” 'ìŠ´ìš°'ë¼ê³  ë¶ˆëŸ¬ì•¼ ë¼.
                
            """),
            MessagesPlaceholder(variable_name="history"),
            ("human", prompt_template.format(context="{context}", question="{question}")),
        ])

        # LLM ì²´ì¸ ìƒì„±
        llm_chain = prompt | GPT

        # Multi-Query ë° Time-Weighted ê¸°ë²• í†µí•©
        def multi_query_and_time_weighted(x):
            context_docs = retriever.get_relevant_documents(x["question"])
            # ìµœê·¼ ë¬¸ì„œ ê¸°ì¤€ìœ¼ë¡œ ìš°ì„  ì²˜ë¦¬ (Time-Weighted)
            sorted_docs = sorted(context_docs, key=lambda doc: doc.metadata['timestamp'], reverse=True)
            return {
                "context": [doc.page_content for doc in sorted_docs],
                "question": x["question"],
                "history": x["history"]
            }

        # RAG ì²´ì¸ ë§Œë“¤ê¸°
        rag_chain = RunnableSequence(
            RunnableLambda(multi_query_and_time_weighted),
            llm_chain
        )

        # ì„¸ì…˜ ê¸°ë¡ì„ í¬í•¨í•œ ì²´ì¸ ì„¤ì •
        chain_with_memory = RunnableWithMessageHistory(
            rag_chain,
            lambda session_id: get_session_history(session_id),
            input_messages_key="question",
            history_messages_key="history",
        )

        # ì‘ë‹µ ìƒì„± ë° ì¶œë ¥
        response = chain_with_memory.invoke(
            {"question": user_input, "history": []},
            config={'configurable': {'session_id': session_id}}
        )

        msg = response.content  # ì‘ë‹µ ë‚´ìš©

        st.session_state["messages"].append(("assistant", msg))
        st.chat_message("assistant").write(msg)
